{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eye-fly/BF-Presets/blob/main/docs/DNN-Lab-3-mnist-again-student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84VetyCaGLyR"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you will implement some of the techniques discussed in the lecture.\n",
        "\n",
        "Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n",
        " * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n",
        " * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence\n",
        "\n",
        "**Task 0.** Implement a numerically stable version of softmax.\n",
        "\n",
        "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block and not even compute the gradient over the softmax values.\n",
        "\n",
        "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
        "\n",
        "**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n",
        "\n",
        "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n",
        "\n",
        "The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFsHblSD0qjC"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P22HqX9AbO1a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from typing import List, Any, Tuple, Optional, Callable, Dict\n",
        "from numpy.typing import NDArray\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9jGPaZhbO2B"
      },
      "outputs": [],
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "\n",
        "def load_mnist(path: str = \".\"):\n",
        "    train_set = datasets.MNIST(path, train=True, download=True)\n",
        "    x_train = train_set.data.numpy()\n",
        "    _y_train = train_set.targets.numpy()\n",
        "\n",
        "    test_set = datasets.MNIST(path, train=False, download=True)\n",
        "    x_test = test_set.data.numpy()\n",
        "    _y_test = test_set.targets.numpy()\n",
        "\n",
        "    x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.0\n",
        "    x_test = x_test.reshape((x_test.shape[0], 28 * 28)) / 255.0\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3gAyqw4bO1p"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: NDArray[float]):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: NDArray[float]):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vdTOyrs0qjC"
      },
      "source": [
        "## Warm-Up\n",
        "Implement a numerically stable version of softmax.  \n",
        "In general, softmax is defined as  \n",
        "$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n",
        "However, taking $e^{1000000}$ can result in NaN.  \n",
        "Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n",
        "\n",
        "Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SigVaW2V0qjC"
      },
      "outputs": [],
      "source": [
        "def unstable_softmax(x: NDArray[float], axis: int = -1):\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def stable_softmax(x: NDArray[float], axis: int = -1):\n",
        "    ## TODO\n",
        "    ###{\n",
        "    e = np.exp(x -x.max())\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "    # pass\n",
        "    ###}\n",
        "\n",
        "\n",
        "### TESTS ###\n",
        "def test_one(x: NDArray[float], y: NDArray[float], proc_fn: Callable[[NDArray[float]], NDArray[float]]):\n",
        "    r = stable_softmax(x)\n",
        "    assert r.shape == x.shape\n",
        "    r = proc_fn(r)\n",
        "    assert r.shape == y.shape\n",
        "    diff = np.mean(np.abs(r - y))\n",
        "    assert diff <= 1e-5, r\n",
        "\n",
        "\n",
        "x1 = np.random.rand(100, 32).astype(np.float64)\n",
        "test_one(x1, np.ones(100), lambda x: x.sum(-1))\n",
        "test_one(x1, unstable_softmax(x1), lambda x: x)\n",
        "\n",
        "\n",
        "x2 = np.ones((100, 32), dtype=np.float64) * 1e6\n",
        "test_one(x2, np.ones_like(x2) / x2.shape[-1], lambda x: x)\n",
        "### TESTS END ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgEA2XRRbO2X"
      },
      "outputs": [],
      "source": [
        "class Network(object):\n",
        "    def __init__(self, sizes: List[int]):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(a @ w + b)\n",
        "\n",
        "        return a\n",
        "\n",
        "    def update_mini_batch(\n",
        "        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n",
        "    ) -> None:\n",
        "        # Update network weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "\n",
        "        nabla_b, nabla_w = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        self.weights = [w - eta * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - eta * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: NDArray[float], y: NDArray[float]\n",
        "    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n",
        "        # For a single input (x,y) return a tuple of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "\n",
        "        assert len(x.shape) == 2  # batch, features\n",
        "        assert len(y.shape) == 2  # batch, classes\n",
        "        assert x.shape[0] == y.shape[0]\n",
        "\n",
        "        # First initialize the list of gradient arrays\n",
        "        delta_nabla_b = []\n",
        "        delta_nabla_w = []\n",
        "\n",
        "        # Then go forward remembering each layer input and value\n",
        "        # before sigmoid activation\n",
        "        layer_input = []\n",
        "        before_act = []\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            layer_input.append(x)\n",
        "            x = x @ w + b\n",
        "            before_act.append(x)\n",
        "            x = sigmoid(x)\n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation\n",
        "        diff = self.cost_derivative(output_activations=x, y=y)\n",
        "        for linp, bef_act, w, b in reversed(\n",
        "            list(zip(layer_input, before_act, self.weights, self.biases))\n",
        "        ):\n",
        "            diff = sigmoid_prime(bef_act) * diff\n",
        "            delta_nabla_w.append(linp.T @ diff)\n",
        "            delta_nabla_b.append(np.sum(diff, axis=0))\n",
        "            diff = diff @ w.T\n",
        "\n",
        "        delta_nabla_w = reversed(delta_nabla_w)\n",
        "        delta_nabla_b = reversed(delta_nabla_b)\n",
        "\n",
        "        # Check shapes\n",
        "        delta_nabla_b = list(delta_nabla_b)\n",
        "        delta_nabla_w = list(delta_nabla_w)\n",
        "        assert len(delta_nabla_b) == len(self.biases), (\n",
        "            len(delta_nabla_b),\n",
        "            len(self.biases),\n",
        "        )\n",
        "        assert len(delta_nabla_w) == len(self.weights), (\n",
        "            len(delta_nabla_w),\n",
        "            len(self.weights),\n",
        "        )\n",
        "        for lid in range(len(self.weights)):\n",
        "            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n",
        "                delta_nabla_b[lid].shape,\n",
        "                self.biases[lid].shape,\n",
        "            )\n",
        "            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n",
        "                delta_nabla_w[lid].shape,\n",
        "                self.weights[lid].shape,\n",
        "            )\n",
        "\n",
        "        return delta_nabla_b, delta_nabla_w\n",
        "\n",
        "    def evaluate(\n",
        "        self, x_test_data: NDArray[float], y_test_data: NDArray[float]\n",
        "    ) -> float:\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [\n",
        "            (\n",
        "                np.argmax(self.feedforward(x_test_data[i].reshape(1, 784)), axis=-1),\n",
        "                np.argmax(y_test_data[i], axis=-1),\n",
        "            )\n",
        "            for i in range(len(x_test_data))\n",
        "        ]\n",
        "        # return accuracy\n",
        "        return np.mean([int((x == y).item()) for (x, y) in test_results]).item()\n",
        "\n",
        "    def cost_derivative(\n",
        "        self, output_activations: NDArray[float], y: NDArray[float]\n",
        "    ) -> NDArray[float]:\n",
        "        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n",
        "        return (output_activations - y) / len(y)  # moved here from update_mini_batch\n",
        "\n",
        "    def optimize(\n",
        "        self,\n",
        "        training_data: Tuple[NDArray[float], NDArray[float]],\n",
        "        epochs: int,\n",
        "        mini_batch_size: int,\n",
        "        eta: float,\n",
        "        test_data: Optional[Tuple[NDArray[float], NDArray[float]]] = None,\n",
        "    ) -> None:\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        epoch_bar = tqdm(range(epochs), desc=\"Epoch\")\n",
        "        for j in epoch_bar:\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[\n",
        "                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n",
        "                ]\n",
        "                y_mini_batch = y_train[\n",
        "                    i * mini_batch_size : (i * mini_batch_size + mini_batch_size)\n",
        "                ]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                epoch_bar.set_description_str(\n",
        "                    \"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test))\n",
        "                )\n",
        "            else:\n",
        "                epoch_bar.set_description_str(\"Epoch: {0}\".format(j))\n",
        "\n",
        "        return self.evaluate(x_test, y_test)\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model_name: str,\n",
        "    model_constructor: Callable[[List[int]], Network],\n",
        "    result_storage: Dict[str, List[Any]],\n",
        "    layers: List[int] = [784, 30, 10],\n",
        "):\n",
        "    # Remove logs from the previous evaluation of the same model\n",
        "    result_storage[\"MODEL\"] = [\n",
        "        m for m in result_storage.get(\"MODEL\", []) if m != model_name\n",
        "    ]\n",
        "    result_storage[\"LR\"] = [\n",
        "        lr\n",
        "        for m, lr in zip(result_storage.get(\"MODEL\", []), result_storage.get(\"LR\", []))\n",
        "        if m != model_name\n",
        "    ]\n",
        "    result_storage[\"ACC_STD\"] = [\n",
        "        acc_std\n",
        "        for m, acc_std in zip(\n",
        "            result_storage.get(\"MODEL\", []), result_storage.get(\"ACC_STD\", [])\n",
        "        )\n",
        "        if m != model_name\n",
        "    ]\n",
        "    result_storage[\"ACC\"] = [\n",
        "        acc\n",
        "        for m, acc in zip(\n",
        "            result_storage.get(\"MODEL\", []), result_storage.get(\"ACC\", [])\n",
        "        )\n",
        "        if m != model_name\n",
        "    ]\n",
        "\n",
        "    for lr in [0.001, 0.01, 0.1, 1.0, 10.0]:\n",
        "\n",
        "        print(f\"Checking with lr = {lr}\")\n",
        "        np.random.seed(42)\n",
        "        accuracy_list = []\n",
        "        for i in range(3):\n",
        "            network = model_constructor(layers)\n",
        "            accuracy = network.optimize(\n",
        "                (x_train, y_train),\n",
        "                epochs=10,\n",
        "                mini_batch_size=100,\n",
        "                eta=lr,\n",
        "                test_data=(x_test, y_test),\n",
        "            )\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "        result_storage[\"MODEL\"].append(model_name)\n",
        "        result_storage[\"LR\"].append(lr)\n",
        "        result_storage[\"ACC_STD\"].append(\n",
        "            f\"{np.mean(accuracy_list) * 100:2.1f}% +- {np.std(accuracy_list) * 100:.1f}\"\n",
        "        )\n",
        "\n",
        "        result_storage[\"ACC\"].append(np.mean(accuracy_list))\n",
        "\n",
        "    df = pd.DataFrame(result_storage).sort_values(\"ACC\", ascending=False)\n",
        "    df = df[[c for c in df.columns if c != \"ACC\"]]\n",
        "    return df\n",
        "\n",
        "\n",
        "RESULTS = {}\n",
        "evaluate_model(\n",
        "    model_name=\"Base\",\n",
        "    model_constructor=lambda x: Network(x),\n",
        "    result_storage=RESULTS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsG24pwg0qjT"
      },
      "source": [
        "## Task 1\n",
        "Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence.   \n",
        "Hint: When implementing backprop it might be easier to consider these two functions as a single block and not even compute the gradient over the softmax values.  \n",
        "If you have problems, please see Appendix A.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iu7_PxV0qjT"
      },
      "outputs": [],
      "source": [
        "class Task1(Network):\n",
        "    def __init__(self, sizes: List[int]):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        super().__init__(sizes=sizes)\n",
        "\n",
        "    def feedforward(self, a: NDArray[float]) -> NDArray[float]:\n",
        "        # Run the network on a single case\n",
        "\n",
        "        ## TODO\n",
        "        ###{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backprop(\n",
        "        self, x: NDArray[float], y: NDArray[float]\n",
        "    ) -> Tuple[List[NDArray[float]], List[NDArray[float]]]:\n",
        "        # For a single input (x,y) return a tuple of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "\n",
        "        assert len(x.shape) == 2  # batch, features\n",
        "        assert len(y.shape) == 2  # batch, classes\n",
        "        assert x.shape[0] == y.shape[0]\n",
        "\n",
        "        ##TODO\n",
        "        ###{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "        # Check shapes\n",
        "        delta_nabla_b = list(delta_nabla_b)\n",
        "        delta_nabla_w = list(delta_nabla_w)\n",
        "        assert len(delta_nabla_b) == len(self.biases), (\n",
        "            len(delta_nabla_b),\n",
        "            len(self.biases),\n",
        "        )\n",
        "        assert len(delta_nabla_w) == len(self.weights), (\n",
        "            len(delta_nabla_w),\n",
        "            len(self.weights),\n",
        "        )\n",
        "        for lid in range(len(self.weights)):\n",
        "            assert delta_nabla_b[lid].shape == self.biases[lid].shape, (\n",
        "                delta_nabla_b[lid].shape,\n",
        "                self.biases[lid].shape,\n",
        "            )\n",
        "            assert delta_nabla_w[lid].shape == self.weights[lid].shape, (\n",
        "                delta_nabla_w[lid].shape,\n",
        "                self.weights[lid].shape,\n",
        "            )\n",
        "\n",
        "        return delta_nabla_b, delta_nabla_w\n",
        "\n",
        "    def cost_derivative(\n",
        "        self, output_activations: NDArray[float], y: NDArray[float]\n",
        "    ) -> NDArray[float]:\n",
        "        assert output_activations.shape == y.shape, (output_activations.shape, y.shape)\n",
        "        ## TODO\n",
        "        ###{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "\n",
        "evaluate_model(\n",
        "    model_name=\"SoftMax\",\n",
        "    model_constructor=lambda x: Task1(x),\n",
        "    result_storage=RESULTS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpIcBjP0qjT"
      },
      "source": [
        "## Task 2\n",
        "Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n",
        "A few notes:\n",
        "* do not regularize the biases\n",
        "* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF99Qr9G0qjT"
      },
      "outputs": [],
      "source": [
        "class Task2(Network):\n",
        "    def __init__(\n",
        "        self, sizes: List[int], l2_factor: float = 1e-4, momentum: float = 0.05\n",
        "    ):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        super().__init__(sizes=sizes)\n",
        "        self.l2_factor = l2_factor\n",
        "        self.momentum = momentum\n",
        "        ## TODO\n",
        "        ####{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "    def update_mini_batch(\n",
        "        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n",
        "    ) -> None:\n",
        "        # Update network weights and biases by applying a single step\n",
        "        # of gradient descent (with momentum and l2 regularization) using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        ## TODO\n",
        "        ###{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "\n",
        "evaluate_model(\n",
        "    model_name=\"L2&Momentum\",\n",
        "    model_constructor=lambda x: Task2(x),\n",
        "    result_storage=RESULTS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hR5s-uq0qjT"
      },
      "source": [
        "## Task 3 (optional)\n",
        "Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.  \n",
        "In case you want to learn about AdamW you can check the official [PyTorch docummentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) that contains pseudocode and the original paper [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).  \n",
        "In Appendix B you can find a simplified version of AdaGrad.  \n",
        "Below you can find brief information regarding the dropout:  \n",
        "\n",
        "During the training phase, we want to make some activations $0$.\n",
        "It is usually implemented by zeroing each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYUOMefi0qjT"
      },
      "outputs": [],
      "source": [
        "class Task3Optimizer(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sizes: List[int],\n",
        "        weight_decay: float = 0.01,\n",
        "        beta_1: float = 0.9,\n",
        "        beta_2: float = 0.95,\n",
        "        eps=1e-5,\n",
        "    ):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        super().__init__(sizes=sizes)\n",
        "        self.weight_decay = weight_decay\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "        self.eps = eps\n",
        "        ## TODO\n",
        "        ####{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "    def update_mini_batch(\n",
        "        self, x_mini_batch: NDArray[float], y_mini_batch: NDArray[float], eta: float\n",
        "    ) -> None:\n",
        "        # Update network weights and biases by applying a single step\n",
        "        # of gradient descent (with momentum and weight decay) using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        ## TODO\n",
        "        ###{\n",
        "        pass\n",
        "        ###}\n",
        "\n",
        "\n",
        "evaluate_model(\n",
        "    model_name=\"Optimizer\",\n",
        "    model_constructor=lambda x: Task3Optimizer(x),\n",
        "    result_storage=RESULTS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCVWEhZR0qjT"
      },
      "outputs": [],
      "source": [
        "# Place for remaining parts of task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqRRzqG_0qjT"
      },
      "source": [
        "## Task 4\n",
        "Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYmkdclQ0qjT"
      },
      "outputs": [],
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybsSgidO0qjT"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFAhzzuc0qjT"
      },
      "source": [
        "## Appendix A - Log-loss and softmax\n",
        "\n",
        "Let's compute the following derivative\n",
        "$$\\frac{\\partial\\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}}$$\n",
        "\n",
        "\n",
        "where\n",
        "$$\n",
        "y_{b, i} = \\begin{cases}\n",
        "1 & \\text{if $b$'th image belongs to $i$'th class}\\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "To do so we can use the chain rule:\n",
        "$$\n",
        "\\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)}  \\frac{\\partial g(x)}{\\partial x}\n",
        "$$\n",
        "the rule for the sum  \n",
        "$$\n",
        "\\frac{\\partial (f(x) + g(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x} +   \\frac{\\partial g(x)}{\\partial x}\n",
        "$$\n",
        "and the fact that the derivative of natural logarithm is\n",
        "$$\n",
        "\\frac{\\partial \\log(x)}{\\partial x} = \\frac{1}{x}\n",
        "$$\n",
        "\n",
        "Using those rules we can observe that indeed:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial  \\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}} = \\sum_{i}{y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Let us review the multiplication rule:\n",
        "$$\n",
        "\\frac{\\partial (f(x)g(x))}{\\partial x} = \\frac{\\partial f(x)}{\\partial x}g(x)  + f(x)\\frac{\\partial g(x)}{\\partial x}\n",
        "$$\n",
        "and that\n",
        "$$\n",
        "\\frac{\\partial e^x}{\\partial x} = e^x\n",
        "$$\n",
        "\n",
        "From the definition:\n",
        "$$\n",
        "\\text{prediction}_{b, f} = \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\n",
        "$$\n",
        "\n",
        "\n",
        "We can observe that for $k \\not= f$\n",
        "$$\n",
        "\\frac{ \\partial \\text{prediction}_{b, k}}{\\partial z_{b, f}}\n",
        "= 0 - \\frac{e^{z_{k, b}} e^{z_{b, f}} }{\\left(\\sum_{i}{e^{z_{b, i}}}\\right)^2}\n",
        "$$\n",
        "and that\n",
        "$$\n",
        "\\frac{ \\partial \\text{prediction}_{b, f}}{\\partial z_{b, f}}\n",
        "= \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}} - \\left(\\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right)^2\n",
        "$$\n",
        "\n",
        "Therefore\n",
        "$$\n",
        "y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}} = \\begin{cases}\n",
        "y_{b,i}\\left(-\\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right) & \\text{if } i \\not= f\\\\\n",
        "y_{b,i}\\left(1 - \\frac{e^{z_{b, f}}}{\\sum_{i}{e^{z_{b, i}}}}\\right) & \\text{if } i = f\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "in other words\n",
        "$$\n",
        "y_{b,i} \\frac{1}{\\text{prediction}_{b, i}} \\frac{ \\partial \\text{prediction}_{b, i}}{\\partial z_{b, f}} = \\begin{cases}\n",
        "y_{b,i}\\left(-\\text{prediction}_{b, f}\\right) & \\text{if } i \\not= f\\\\\n",
        "y_{b,i}\\left(1 - \\text{prediction}_{b, f}\\right) & \\text{if } i = f\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "As we just sum over $i$, and for each $b$ there is exactly one $i$ such that $y_{b, i} = 1$ and for $j \\not= i$ $y_{b, j} = 0$, therefore  in the end we have that\n",
        "$$\n",
        "\\frac{\\partial  \\left(\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}\\right)}{\\partial z_{b, f}} = y_{b, f} - \\text{prediction}_{b, f}\n",
        "$$\n",
        "what can be written as:\n",
        "$$\n",
        "y - prediction\n",
        "$$\n",
        "\n",
        "In the code, we are going to use\n",
        "$$\n",
        "prediction - y\n",
        "$$\n",
        "as our loss is\n",
        "$$-\\sum_{b}\\sum_{i}{y_{b, i}\\log{(\\text{prediction}_{b, i}})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suiAzKzk0qjT"
      },
      "source": [
        "## Appendix B - Adagrad (simplified version)\n",
        "\n",
        "Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n",
        "For parameter $p_i$ we maintain an array $G_i$ (can be set to $0$ initially).\n",
        "Let $\\text{LOSS}$ be our loss without L2.   \n",
        "We update $G_i$ and $p_i$ each training step as follows:  \n",
        "$$\n",
        "G_i = G_i +  \\left(\\frac{\\partial \\text{LOSS}}{\\partial p_i}\\right)^2\\\\\n",
        "p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\text{LOSS}}{\\partial p_i}\n",
        "$$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}